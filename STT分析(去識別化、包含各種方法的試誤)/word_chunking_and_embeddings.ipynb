{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import chardet\n",
    "from src.utils.text_chunking import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_encoding(file_path):\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        result = chardet.detect(f.read())\n",
    "        return result['encoding']\n",
    "    \n",
    "def read_txt(file_path):\n",
    "    encoding = detect_encoding(file_path)\n",
    "    with open(file_path, 'r', encoding=encoding) as f:\n",
    "        content = f.read()\n",
    "        return content\n",
    "\n",
    "def read_file(folder_path):\n",
    "    corpus = {}\n",
    "    files = [f for f in os.listdir(folder_path) if f.endswith(\".txt\")]\n",
    "    for file in files:\n",
    "        file_path = os.path.join(folder_path, file)\n",
    "        content = read_txt(file_path)\n",
    "        corpus[file] = content\n",
    "\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = read_file(folder_path =\"processed_data\")\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_definition = corpus[\"data.txt\"]\n",
    "FAQ = corpus['FAQ.txt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "record = [\"T00005025.script.txt\", \"T00005026+27.script.txt\", \"T00005026.script.txt\", \"T00005027.script.txt\", \"T00005028.script.txt\"]\n",
    "record_text = {key: corpus[key] for key in record if key in corpus}\n",
    "record_text = \" \".join(record_text.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunking = Chunking(text=word_definition)\n",
    "word_definition = chunking.text_chunking(word_definition, separators=['名詞解釋', '。'], chunk_size=50, chunk_overlap=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "faq = chunking.text_chunking(FAQ, separators=[\"KEYWORD\", \"KEYWORD\", \"KEYWORD\", \"KEYWORD\", \"KEYWORD\", \"KEYWORD\", \"KEYWORD\"], chunk_size=100, chunk_overlap=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "insurance = [\"invest1.txt\",\"invest2.txt\",\"invest3.txt\",\"normal1.txt\",\"normal3.txt\",\"normal3.txt\"]\n",
    "insurance_text = {key: corpus[key] for key in insurance if key in corpus}\n",
    "insurance_text = \" \".join(insurance_text.values())\n",
    "\n",
    "chunking = Chunking(text=insurance_text)\n",
    "\n",
    "insurance_chunk = chunking.text_chunking(insurance_text,\n",
    "                                         separators=[\"KEYWORD\", \"KEYWORD\", \"KEYWORD\\KEYWORD\", \"KEYWORD\", \"KEYWORD\", \"KEYWORD\", \"KEYWORD\", \"KEYWORD\", \"？\"],\n",
    "                                         chunk_size=50,\n",
    "                                         chunk_overlap=10)\n",
    "insurance_chunk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "record_chunk = chunking.text_chunking(record_text, separators=[\"。\", \"，\"], chunk_size=100, chunk_overlap=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.embeddings import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "we = WordEmbedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_definition_embeddings = we.embedding(text=word_definition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"word_definition_embeddings\", \"wb\") as f:\n",
    "    pickle.dump(word_definition_embeddings, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "faq_embeddings = we.embedding(text=faq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"faq_embeddings\", \"wb\") as f:\n",
    "    pickle.dump(faq_embeddings, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "insurance_chunk_embeddings = we.embedding(text=insurance_chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"insurance_chunk_embeddings\", \"wb\") as f:\n",
    "    pickle.dump(insurance_chunk_embeddings, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "record_chunk_embeddings = we.embedding(text=record_chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"record_chunk_embeddings\", \"wb\") as f:\n",
    "    pickle.dump(record_chunk_embeddings, f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "幾條規則分開做embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "Rule = {}\n",
    "\n",
    "for i in [\"KEYWORD\", \"KEYWORD\", \"KEYWORD\\KEYWORD\", \"KEYWORD\", \"KEYWORD\", \"KEYWORD\", \"KEYWORD\", \"KEYWORD\"]:\n",
    "    Rule[f\"{i}\"] = []\n",
    "    for text in insurance_chunk:\n",
    "        if i in text:\n",
    "            Rule[f\"{i}\"].append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_words = ['請問正確嗎', '請問您是否同意', '請問您清楚嗎', '請問是否正確', '請問您是否同意']\n",
    "\n",
    "def word_filter(data, remove_words):\n",
    "    pattern = re.compile('|'.join(map(re.escape, remove_words)))\n",
    "    for key, sentences in data.items():\n",
    "        data[key] = [pattern.sub('', sentence) for sentence in sentences]\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_Rule = word_filter(Rule, remove_words=remove_words)\n",
    "filtered_Rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "Rule_embedding_folder = \"Rule_embedding\"\n",
    "os.makedirs(Rule_embedding_folder, exist_ok=True)\n",
    "\n",
    "name_mapping = {rule: rule.replace(\"\\n\",\"\") for rule in Rule.keys()}\n",
    "\n",
    "for rule, contents in filtered_Rule.items():\n",
    "    rule_embeddings = []\n",
    "    for content in contents:\n",
    "        embeddings = we.embedding(content)\n",
    "        rule_embeddings.append(embeddings)\n",
    "    safe_rule_name = name_mapping[rule]    \n",
    "    filename = os.path.join(Rule_embedding_folder, f\"{safe_rule_name}.pkl\")\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(rule_embeddings, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
