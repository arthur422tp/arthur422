{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from src.utils.text_chunking import *\n",
    "from sentence_transformers import util\n",
    "from src.utils.embeddings import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'C:\\path\\to\\word_definition_embeddings','rb') as f:\n",
    "    word_definition_embeddings = pickle.load(f)\n",
    "\n",
    "with open(r\"C:\\path\\to\\faq_embeddings\", 'rb') as f:\n",
    "    faq_embeddings =pickle.load(f)\n",
    "\n",
    "with open(r\"C:\\path\\to\\insurance_chunk_embeddings\", 'rb') as f:\n",
    "    insurance_chunk_embeddings =pickle.load(f)\n",
    "\n",
    "with open(r\"C:\\path\\to\\record_chunk_embeddings\", 'rb') as f:\n",
    "    record_chunk_embeddings = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import chardet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_encoding(file_path):\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        result = chardet.detect(f.read())\n",
    "        return result['encoding']\n",
    "    \n",
    "def read_txt(file_path):\n",
    "    encoding = detect_encoding(file_path)\n",
    "    with open(file_path, 'r', encoding=encoding) as f:\n",
    "        content = f.read()\n",
    "        return content\n",
    "\n",
    "def read_file(folder_path):\n",
    "    corpus = {}\n",
    "    files = [f for f in os.listdir(folder_path) if f.endswith(\".txt\")]\n",
    "    for file in files:\n",
    "        file_path = os.path.join(folder_path, file)\n",
    "        content = read_txt(file_path)\n",
    "        corpus[file] = content\n",
    "\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = read_file(folder_path =\"processed_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = corpus[\"data.txt\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "record_text = corpus['T00005025.script.txt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.retrievals import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = Retrieval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_definition = corpus[\"data.txt\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunking = Chunking(text=word_definition)\n",
    "word_definition = chunking.text_chunking(word_definition, separators=['名詞解釋', '。'], chunk_size=50, chunk_overlap=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "insurance = [\"invest1.txt\",\"invest2.txt\",\"invest3.txt\",\"normal1.txt\",\"normal3.txt\",\"normal3.txt\"]\n",
    "insurance_text = {key: corpus[key] for key in insurance if key in corpus}\n",
    "insurance_text = \" \".join(insurance_text.values())\n",
    "chunking = Chunking(text=insurance_text)\n",
    "\n",
    "insurance_chunk = chunking.text_chunking(insurance_text,\n",
    "                                         separators=[\"KEYWORD\", \"KEYWORD\", \"KEYWORD\\KEYWORD\", \"KEYWORD\", \"KEYWORD\", \"KEYWORD\", \"KEYWORD\", \"KEYWORD\", \"？\"],\n",
    "                                         chunk_size=50,\n",
    "                                         chunk_overlap=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"path\\to\\caption.txt\", encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "texts = [line.split(\" \", 1)[1].strip() for line in lines]\n",
    "texts = \"\".join(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = r.build_index(embeddings=insurance_chunk_embeddings)\n",
    "q = r.retrieval(query=texts, text=insurance_chunk, index=index, k=30, threshold=0.88)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "Rule = {}\n",
    "\n",
    "for i in [\"KEYWORD\", \"KEYWORD\", \"KEYWORD\\KEYWORD\", \"KEYWORD\", \"KEYWORD\", \"KEYWORD\", \"KEYWORD\", \"KEYWORD\"]:\n",
    "    Rule[f\"{i}\"] = []\n",
    "    for text in insurance_chunk:\n",
    "        if i in text:\n",
    "            Rule[f\"{i}\"].append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_words = ['請問正確嗎', '請問您是否同意', '請問您清楚嗎', '請問是否正確', '請問您是否同意']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_filter(data, remove_words):\n",
    "    remove_words = ['請問正確嗎', '請問您是否同意', '請問您清楚嗎', '請問是否正確', '請問您是否同意']\n",
    "    pattern = re.compile('|'.join(map(re.escape, remove_words)))\n",
    "    for key, sentences in data.items():\n",
    "        data[key] = [pattern.sub('', sentence) for sentence in sentences]\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_Rule = word_filter(Rule, remove_words=remove_words)\n",
    "filtered_Rule"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "錄音檔vs規則"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"path\\to\\caption.txt\", encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "record_texts = [line.split(\" \", 1)[1].strip() for line in lines]\n",
    "record_texts = \"\".join(record_texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def load_rule_embeddings(folder):\n",
    "    rule_embeddings = {}\n",
    "    for rule_file in os.listdir(folder):\n",
    "        if rule_file.endswith(\".pkl\"):\n",
    "            rule_name = rule_file.replace(\".pkl\", \"\")\n",
    "            with open(os.path.join(folder, rule_file), \"rb\") as f:\n",
    "                rule_embeddings[rule_name] = pickle.load(f)\n",
    "    return rule_embeddings\n",
    "\n",
    "def calculate_sim(vector, matrix):\n",
    "    return cosine_similarity(vector.reshape(1, -1), matrix).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "rule_folder = r\"C:\\Users\\YT0283\\Desktop\\stt_anaysis\\Rule_embedding\"\n",
    "rule_embeddings = load_rule_embeddings(rule_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "we = WordEmbedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regex_test(max_scores,  texts_chunks):\n",
    "    key_word = {'KEYWORD'}\n",
    "    key_extract = {k: embeddings for k, embeddings in max_scores.items() if k in key_word} \n",
    "    pattern = r\"十日|10日|十天|10天\"\n",
    "    return bool(re.search(pattern, texts_chunks[key_extract['契撤期']['chunk']-1]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(folder, record_texts):\n",
    "    chunking = Chunking(text=record_texts)\n",
    "    we = WordEmbedding()\n",
    "    texts_chunks = chunking.text_chunking(record_texts,\n",
    "                                         separators=[\"嗎\", \"是否確認清楚\", \"瞭解瞭解\", \"同意同意\", \"了解了解\", \"請問\"],\n",
    "                                         chunk_size=50,\n",
    "                                         chunk_overlap=10)\n",
    "    \n",
    "    rule_embeddings = load_rule_embeddings(folder)\n",
    "    max_scores = {rule: {\"score\": 0, \"chunk\": None} for rule in rule_embeddings.keys()}\n",
    "    for i, chunk in enumerate(texts_chunks):\n",
    "        chunk_embedding = we.embedding(chunk)\n",
    "        print(\"-\"*50)\n",
    "        print(f\"chunk{i}:{chunk}\")\n",
    "        for rule, embeddings in rule_embeddings.items():\n",
    "            rule_embeddings_matrix = np.array(embeddings)\n",
    "            rule_embeddings_matrix = rule_embeddings_matrix.mean(axis=0)\n",
    "            sim = calculate_sim(chunk_embedding, rule_embeddings_matrix.reshape(1,-1))\n",
    "            print(f\"錄音檔與{rule}之相似度:{sim[0]:.4f}\")\n",
    "            if sim[0] > max_scores[rule][\"score\"]:\n",
    "                max_scores[rule][\"score\"] = sim[0]\n",
    "                max_scores[rule][\"chunk\"] = i+1\n",
    "\n",
    "            \n",
    "    print(\"=\"*50)\n",
    "    print(f\"規則分數:\")\n",
    "    for rule, data in max_scores.items():\n",
    "        print(f\"{rule}:{data['score']:.4f} in chunk{data['chunk']}\")\n",
    "    \n",
    "    if regex_test(max_scores,  texts_chunks) == True:\n",
    "        print(\"=\"*50)\n",
    "        print(\"內容正確\")\n",
    "    if regex_test(max_scores,  texts_chunks) == False:\n",
    "        print(\"=\"*50)\n",
    "        print(\"內容可能有誤\")\n",
    "        print(f\"請檢查chunk{max_scores['KEYWORD']['chunk']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run(folder=rule_folder, record_texts=record_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topic_classification(folder, record_texts):\n",
    "    chunking = Chunking(text=record_texts)\n",
    "    we = WordEmbedding()\n",
    "    texts_chunks = chunking.text_chunking(record_texts,\n",
    "                                         separators=[\"嗎\", \"瞭解瞭解\", \"同意同意\", \"了解了解\"],\n",
    "                                         chunk_size=50,\n",
    "                                         chunk_overlap=10)\n",
    "    \n",
    "    rule_embeddings = load_rule_embeddings(folder)\n",
    "    max_scores = {chunk : {\"score\": 0, \"rule\": None} for chunk in texts_chunks}\n",
    "    for i, chunk in enumerate(texts_chunks):\n",
    "        chunk_embedding = we.embedding(chunk)\n",
    "        #print(\"-\"*50)\n",
    "        #print(f\"chunk{i+1}:{chunk}\")\n",
    "        for rule, embeddings in rule_embeddings.items():\n",
    "            rule_embeddings_matrix = np.array(embeddings)\n",
    "            rule_embeddings_matrix = rule_embeddings_matrix.mean(axis=0)\n",
    "            sim = calculate_sim(chunk_embedding, rule_embeddings_matrix.reshape(1,-1))\n",
    "            #print(f\"錄音檔與{rule}之相似度:{sim[0]:.4f}\")\n",
    "            if sim[0] > max_scores[chunk][\"score\"]:\n",
    "                max_scores[chunk][\"score\"] = sim[0]\n",
    "                max_scores[chunk][\"rule\"] = rule\n",
    "    return max_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_dict = topic_classification(folder=rule_folder, record_texts=record_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for text_dict in topic_dict:\n",
    "    for key, value in topic_dict.items():\n",
    "        topic = value['rule']\n",
    "        if topic in filtered_Rule:\n",
    "            value[\"topic\"] = {topic: filtered_Rule[topic]}\n",
    "\n",
    "\n",
    "topic_dict"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk, rule in topic_dict.items():\n",
    "    print(rule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "url = \"http://ditgpu01.aegon.com.tw/ollama-test/api/chat\"\n",
    "\n",
    "for chunk, rule in topic_dict.items():\n",
    "  prompt = f\"\"\"\n",
    "  你現在是一個對話檢查員，檢查客服人員是否在與顧客的對話中有提到銷售商品時的應注意事項。\n",
    "\n",
    "  首先你會收到以下規則以及這些規則的範文:\n",
    "  1.'KEYWORD'\n",
    "  2.'KEYWORD'\n",
    "  3.'KEYWORD'\n",
    "  4.'KEYWORD'\n",
    "  5.'KEYWORD'\n",
    "  6.'KEYWORD'\n",
    "  7.'聲明'\n",
    "  8.'標的說明'\n",
    "\n",
    "  再來會收到對話紀錄的一個段落，請比對並確認段落是否清楚地表達了規則。\n",
    "\n",
    "  注意到：範文可能同時涉及多個規則，因此段落有至少提及一個規則即為正確。\n",
    "\n",
    "  並遵照以下格式以繁體中文輸出:\n",
    "  1.**是否正確?(是/否)**\n",
    "  若1.的答案為否\n",
    "  請回應:\n",
    "  **建議修正:**\n",
    "\n",
    "  段落:{chunk}\n",
    "  規則:{rule}\n",
    "\n",
    "  \"\"\"\n",
    "  payload = {\n",
    "    \"model\": \"llama3.1\",\n",
    "    \"messages\": [\n",
    "      {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": prompt\n",
    "      }\n",
    "    ],\n",
    "    \"stream\": False\n",
    "  }\n",
    "\n",
    "  headers = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "  response = requests.post(url, json=payload, headers=headers)\n",
    "\n",
    "  formatted = json.dumps(response.json(), indent=4, ensure_ascii=False)\n",
    "\n",
    "  print(chunk)\n",
    "  print(formatted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
